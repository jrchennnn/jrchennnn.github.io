<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description"
    content="We present CODERS, a one-stage approach for Category-level Object Detection, pose Estimation and Reconstruction from Stereo images.">
  <meta property="og:title" content="Category-level Object Detection, Pose Estimation and Reconstruction from Stereo Images" />
  <meta property="og:description"
    content="We present CODERS, a one-stage approach for Category-level Object Detection, pose Estimation and Reconstruction from Stereo images." />
  <meta property="og:url" content="https://xingyoujun.github.io/coders" />
  <meta property="og:image" content="static/images/coders_pipeline.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="400" />

  <meta name="twitter:title" content="Category-level Object Detection, Pose Estimation and Reconstruction from Stereo Images">
  <meta name="twitter:description" content="We present CODERS, a one-stage approach for Category-level Object Detection, pose Estimation and Reconstruction from Stereo images.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/coders_pipeline.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Stereo vision, Category-level Pose Estimation, Shape Reconstruction">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>coders</title>
  <!-- Favicon generated by https://redketchup.io/favicon-generator -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon/favicon.ico">
  <link rel="apple-touch-icon" sizes="180x180" href="static/images/favicon/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="static/images/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="static/images/favicon/favicon-16x16.png">
  <link rel="manifest" href="static/images/favicon/site.webmanifest">

  <link rel="stylesheet" href="static/css/bootstrap.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bootstrap.bundle.min.js"></script>
</head>

<body>
  <div class="container">
    <!-- Title -->
    <h1 class="pt-5 title mb-4">Category-level Object Detection, Pose Estimation and Reconstruction from Stereo Images</h1>

    <div class="mb-4" style="text-align: center; font-size: 1.5rem;">
      ECCV 2024
    </div>

    <div class="mb-2" style="font-size: larger; text-align: center;">
      <span class="author-block">
        <a href="https://xingyoujun.github.io/">Chuanrui Zhang</a><sup>1,2*</sup></span>,&nbsp;
      <span class="author-block">
        <a href="https://ygling2008.github.io/">Yonggen Ling</a><sup>2*</sup></span>,&nbsp;
      <span class="author-block">
        Minglei Lu<sup>2</sup></span>,&nbsp;
      <span class="author-block">
        <a href="https://minghanqin.github.io/">Minghan Qin</a><sup>1</sup></span>,&nbsp;<br>
      <span class="author-block">
        Haoqian Wang<sup>1</sup></span>&nbsp;
      <span class="author-block">
    </div>
    <div class="mb-4" style="text-align: center;">
      <span><sup>1</sup>Tsinghua University</span>,&nbsp;
      <span><sup>2</sup>Tencent Robotics X</span>&nbsp;
    </div>
    <div class="mb-4" style="text-align: center;">
      <span>* Equal Contribution</span>&nbsp;
    </div>

    <div class="w-100 d-flex flex-row justify-content-center mt-4 gap-2">
      <!-- Paper PDF -->
      <a href="https://arxiv.org/abs/2407.06984" target="_blank" class="btn btn-dark" role="button">
        <span class="icon">
          <i class="fas fa-file-pdf"></i>
        </span>
        <span>Paper</span>
      </a>
    
      <!-- Code -->
      <a href="#todo" target="_blank" class="btn btn-dark" role="button">
        <span class="icon">
          <i class="fab fa-github"></i>
        </span>
        <span>Code coming soon</span>
      </a>

      <!-- Dataset -->
      <a href="https://huggingface.co/datasets/xingyoujun/ss3d" target="_blank" class="btn btn-dark" role="button">
        <span class="icon">
          <i class="fa fa-database"></i>
        </span>
        <span>Dataset</span>
      </a>

      <!-- Video -->
      <a href="https://www.youtube.com/watch?v=MxZdNAy4EA4" target="_blank" class="btn btn-dark" role="button">
        <span class="icon">
          <i class="fab fa-youtube"></i>
        </span>
        <span>Video</span>
      </a>

      <!-- Pre-trained Models -->
      <!-- <a href="https://drive.google.com/drive/folders/14_E_5R6ojOWnLSrSVLVEMHnTiKsfddjU" target="_blank"
        class="btn btn-dark" role="button">
        <span class="icon">
          <i class="fas fa-database"></i>
        </span>
        <span>Pre-trained Models</span>
      </a> -->
    </div>

    <!-- Teaser -->
    <!-- <div class="w-100 my-4">
      <video class="w-100 d-block" autoplay controls muted loop>
        <source src="https://www.youtube.com/watch?v=MxZdNAy4EA4">
      </video>
    </div> -->

    <div class="main-contain">
      <!-- TL;DR -->
      <h2>TL;DR</h2>
      <div class="alert alert-success tldr mb-4">
        We present CODERS, a one-stage approach for category-level object detection, pose estimation and reconstruction from stereo images.
      </div>

      <!-- Abstract -->
      <h2>Abstract</h2>
      <p class="mb-4" id="abstract">
        We study the 3D object understanding task for manipulating everyday objects with different material properties (diffuse, specular, transparent and mixed). 
        Existing monocular and RGB-D methods suffer from scale ambiguity due to missing or imprecise depth measurements. 
        We present CODERS, a one-stage approach for Category-level Object Detection, pose Estimation and Reconstruction from Stereo images. 
        The base of our pipeline is an implicit stereo matching module that combines stereo image features with 3D position information. 
        Concatenating this presented module and the following transform-decoder architecture leads to end-to-end learning of multiple tasks required by robot manipulation. 
        Our approach significantly outperforms all competing methods in the public TOD dataset. 
        Furthermore, trained on simulated data, CODERS generalize well to unseen category-level object instances in real-world robot manipulation experiments. 
      </p>

      <h2>Architecture</h2>
      <figure class="mb-4">
        <img src="static/images/coders_pipeline.png" class="img-fluid teaser" alt="architecture" />
        <figcaption style="font-size: smaller;text-align: justify;"><b>Overview of Coders.</b> We present a single-stage network capable of processing multiple unknown objects, 
          outputting detections, classes, 6D poses and 3D shapes concurrently. Using stereo images as input, 
          our network generates stereo-aware features for easier alignment in implicit feature space. 
          During the transformer decoder stage, object queries interact with 3D stereo-aware features, yielding object embeddings. 
          These object embeddings are used to infer the category, pose and shape of objects using corresponding modules, which serve as the final output. 
          In the Implicit Stereo Matching module, CT denotes coordinate transformer.</figcaption>
      </figure>

      <!-- Comparisons -->
      <h2>Comparisons with the State-of-the-art</h2>
      <p>We present qualitative comparisons with the following state-of-the-art models:</p>
      <ul>
        <li><a href="https://appsrv.cse.cuhk.edu.hk/~kaichen/stereopose.html">stereopose</a>: The best method on TOD dataset.</li>
      </ul>
      <img src="static/images/comparision_with_sota.png" class="img-fluid w-100 mt-2 mb-3" alt="comparison on TOD dataset" />
      <div class="border w-100 mb-4">
        <video class="w-100 d-block" autoplay controls muted loop>
          <source src="static/videos/tod_video.mp4" type="video/mp4">
        </video>
      </div>

      <!-- manipulation -->
      <h2>Real World Test</h2> 
      <p>Our Coders can handle everyday objects with various surface properties.</p>
      <div class="border w-100 mb-4">
        <video class="w-100 d-block" autoplay controls muted loop>
          <source src="static/videos/real_world.mp4" type="video/mp4">
        </video>
      </div>

      <!-- manipulation -->
      <h2>Robot Manipulation </h2> 
      <p>Our Coders can provide reliable estimation results for robot manipulation.</p>
      <div class="border w-100 mb-4">
        <video class="w-100 d-block" autoplay controls muted loop>
          <source src="static/videos/robot_manipulation.mp4" type="video/mp4">
        </video>
      </div>

    <!-- <h2>BibTeX</h2>
    <pre class="mb-4"><code>@article{chen2024mvsplat,
    title   = {MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images},
    author  = {Chen, Yuedong and Xu, Haofei and Zheng, Chuanxia and Zhuang, Bohan and Pollefeys, Marc and Geiger, Andreas and Cham, Tat-Jen and Cai, Jianfei},
    journal = {arXiv preprint arXiv:2403.14627},
    year    = {2024},
}</code></pre> -->

    </div>

    <!-- Footer -->
    <footer class="border-top mt-5 py-4">
      This page's code uses elements from this <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
        target="_blank">Academic Project Page
        Template</a>.
    </footer>
  </div>
</body>

</html>