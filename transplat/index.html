<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description"
    content="We present TranSplat, a transformer-based approach for generalizable 3D gaussian splatting from sparse multi-view images.">
  <meta property="og:title" content="TranSplat: Generalizable 3D Gaussian Splatting from Sparse Multi-View Images with Transformers" />
  <meta property="og:description"
    content="We present TranSplat, a transformer-based approach for generalizable 3D gaussian splatting from sparse multi-view images." />
  <meta property="og:url" content="https://xingyoujun.github.io/transplat" />
  <meta property="og:image" content="static/images/teaser.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="400" />

  <meta name="twitter:title" content="TranSplat: Generalizable 3D Gaussian Splatting from Sparse Multi-View Images with Transformers">
  <meta name="twitter:description" content="We present TranSplat, a transformer-based approach for generalizable 3D gaussian splatting from sparse multi-view images.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/teaser.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="3D Reconstruction, Generalizable Gaussian Splatting, Transformer">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>TranSplat</title>
  <!-- Favicon generated by https://redketchup.io/favicon-generator -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon/favicon.ico">
  <link rel="apple-touch-icon" sizes="180x180" href="static/images/favicon/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="static/images/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="static/images/favicon/favicon-16x16.png">
  <link rel="manifest" href="static/images/favicon/site.webmanifest">

  <link rel="stylesheet" href="static/css/bootstrap.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bootstrap.bundle.min.js"></script>
</head>

<body>
  <div class="container">
    <!-- Title -->
    <h1 class="pt-5 title mb-4">TranSplat: Generalizable 3D Gaussian Splatting from Sparse Multi-View Images with Transformers</h1>

    <div class="mb-4" style="text-align: center; font-size: 1.5rem;">
      AAAI 2025
    </div>

    <div class="mb-2" style="font-size: larger; text-align: center;">
      <span class="author-block">
        <a href="https://xingyoujun.github.io/">Chuanrui Zhang</a><sup>1*</sup></span>,&nbsp;
      <span class="author-block">
        <a href="https://heiheishuang.xyz/">Yingshuang Zou</a><sup>1*</sup></span>,&nbsp;
      <span class="author-block">
        <a href="https://lizhuoling.github.io/">Zhuoling Li</a><sup>2</sup></span>,&nbsp;  
      <span class="author-block">
        Minmin Yi<sup>3</sup></span>,&nbsp;
      <span class="author-block">
        Haoqian Wang<sup>1</sup></span>&nbsp;
      <span class="author-block">
    </div>
    <div class="mb-4" style="text-align: center;">
      <span><sup>1</sup>Tsinghua University</span>,&nbsp;
      <span><sup>2</sup>The University of Hong Kong</span>&nbsp;
      <span><sup>3</sup>E-surfing Vision Technology Co., Ltd</span>&nbsp;
    </div>
    <div class="mb-4" style="text-align: center;">
      <span>* Equal Contribution</span>&nbsp;
    </div>

    <div class="w-100 d-flex flex-row justify-content-center mt-4 gap-2">
      <!-- Paper PDF -->
      <a href="https://arxiv.org/abs/2408.13770" target="_blank" class="btn btn-dark" role="button">
        <span class="icon">
          <i class="fas fa-file-pdf"></i>
        </span>
        <span>Paper</span>
      </a>
    
      <!-- Code -->
      <a href="https://github.com/xingyoujun/transplat" target="_blank" class="btn btn-dark" role="button">
        <span class="icon">
          <i class="fab fa-github"></i>
        </span>
        <span>Code</span>
      </a>

      <!-- Dataset -->
      <!-- <a href="https://huggingface.co/datasets/xingyoujun/ss3d" target="_blank" class="btn btn-dark" role="button">
        <span class="icon">
          <i class="fa fa-database"></i>
        </span>
        <span>Dataset</span>
      </a> -->

      <!-- Video -->
      <!-- <a href="https://www.youtube.com/watch?v=MxZdNAy4EA4" target="_blank" class="btn btn-dark" role="button">
        <span class="icon">
          <i class="fab fa-youtube"></i>
        </span>
        <span>Video</span>
      </a> -->

      <!-- Pre-trained Models -->
      <a href="https://huggingface.co/xingyoujun/transplat" target="_blank"
        class="btn btn-dark" role="button">
        <span class="icon">
          <i class="fas fa-database"></i>
        </span>
        <span>Pre-trained Models</span>
      </a>
    </div>

    <!-- Teaser -->
    <div class="w-100 my-4">
      <figure class="mb-4">
        <img src="static/images/teaser.png" class="img-fluid teaser" alt="architecture" />
      </figure>
    </div>

    <div class="main-contain">
      <!-- TL;DR -->
      <h2>TL;DR</h2>
      <div class="alert alert-success tldr mb-4">
        We present TranSplat, a transformer-based approach for generalizable 3D gaussian splatting from sparse multi-view images.
      </div>

      <!-- Abstract -->
      <h2>Abstract</h2>
      <p class="mb-4" id="abstract">
        Compared with previous 3D reconstruction methods like Nerf, recent Generalizable 3D Gaussian Splatting (G-3DGS) methods demonstrate impressive efficiency even in the sparse-view setting. 
        However, the promising reconstruction performance of existing G-3DGS methods relies heavily on accurate multi-view feature matching, which is quite challenging. 
        Especially for the scenes that have many non-overlapping areas between various views and contain numerous similar regions, the matching performance of existing methods is poor and the reconstruction precision is limited. 
        To address this problem, we develop a strategy that utilizes a predicted depth confidence map to guide accurate local feature matching. 
        In addition, we propose to utilize the knowledge of existing monocular depth estimation models as prior to boost the depth estimation precision in non-overlapping areas between views. 
        Combining the proposed strategies, we present a novel G-3DGS method named TranSplat, which obtains the best performance on both the RealEstate10K and ACID benchmarks while maintaining competitive speed and presenting strong cross-dataset generalization ability.
      </p>

      <h2>Architecture</h2>
      <figure class="mb-4">
        <img src="static/images/pipeline.png" class="img-fluid teaser" alt="architecture" />
        <figcaption style="font-size: smaller;text-align: justify;"><b>Overview of TranSplat.</b> 
          Our method takes multi-view images as input and first extracts image features and monocular depth priors. 
          Next, the coarse-to-fine matching stage is used to obtain a geometry-consistent depth distribution for each view. 
          Specifically, we compute multi-view feature similarities using our proposed Depth-Aware Deformable Matching Transformer module. 
          The Depth Refine U-Net is then employed to further refine the depth prediction. 
          Finally, we predict pixel-wise 3D Gaussian parameters to render novel views.</figcaption>
      </figure>

      <!-- Comparisons -->
      <h2>Comparisons with the State-of-the-art</h2>
      <p>We present qualitative comparisons with the following state-of-the-art models:</p>
      <ul>
        <li><a href="https://davidcharatan.com/pixelsplat/">pixelSplat</a>: The latest feed-forward 3D Gaussians model with epipolar Transformer.</li>
      </ul>
      <ul>
        <li><a href="https://donydchen.github.io/mvsplat/">MVSplat</a>: The latest SOTA feed-forward 3D Gaussians model with costvolume.</li>
      </ul>
      <img src="static/images/sota_comparsion.png" class="img-fluid w-100 mt-2 mb-3" alt="SOTA comparisons" />
      <!-- <div class="border w-100 mb-4">
        <video class="w-100 d-block" autoplay controls muted loop>
          <source src="static/videos/tod_video.mp4" type="video/mp4">
        </video>
      </div> -->

      <!-- pc results -->
      <h2>Geometry Reconstruction</h2> 
      <p>Our TranSplat generates impressive 3D Gaussian primitives which is atrributed to our high-quality depth estimation results.</p>
      <img src="static/images/pc_result.png" class="img-fluid w-100 mt-2 mb-3" alt="SOTA comparisons" />

      <!-- manipulation -->
      <h2>Cross-dataset Generalization </h2> 
      <p>Our proposed TranSplat demonstrates significant superiority in generalizing to out-of-distribution novel scenes.</p>
      <img src="static/images/cross_dataset.png" class="img-fluid w-100 mt-2 mb-3" alt="cross-dataset comparisons" />

    <!-- <h2>BibTeX</h2>
    <pre class="mb-4"><code>@article{chen2024mvsplat,
    title   = {MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images},
    author  = {Chen, Yuedong and Xu, Haofei and Zheng, Chuanxia and Zhuang, Bohan and Pollefeys, Marc and Geiger, Andreas and Cham, Tat-Jen and Cai, Jianfei},
    journal = {arXiv preprint arXiv:2403.14627},
    year    = {2024},
}</code></pre> -->

    </div>

    <!-- Footer -->
    <footer class="border-top mt-5 py-4">
      This page's code uses elements from this <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
        target="_blank">Academic Project Page
        Template</a>.
    </footer>
  </div>
</body>

</html>
