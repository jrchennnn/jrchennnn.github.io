---
permalink: /
title: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

# ğŸ§‘ About me

I am an incoming Ph.D. candidate at the [Shanghai Innovation Institute (SII)](https://www.sii.edu.cn/) and [Fudan University (FDU)](https://www.fudan.edu.cn/en/), concurrently completing my undergraduate studies in Computer Science and Technology at the [Harbin Institute of Technology, Shenzhen (HITSZ)](https://www.hitsz.edu.cn/), with an expected graduation in 2026. Currently, I am a Research Intern at the [Tencent Hunyuan3D team](https://3d.hunyuan.tencent.com/), dedicated to developing next-generation efficient architectures for 3D generation.

<!-- [CV](files/cv_en.pdf)   [ç®€å†](files/cv_zh.pdf) -->

My current research interests include:
- **Embodied Intelligence & Robotics**
- **3D Vision**, particularly in 3D representation, with a focus on generalizable methods
- **Efficient AI**, spanning both training and inference, with a focus on techniques such as Parameter-Efficient Fine-Tuning
- **Generative AI**, particularly in 3D generation, video generation, and world models
  
<br>

# ğŸ”¥ News
* 2026.01:  ğŸ‰ğŸ‰ 1 paper accepted to ICLR 2026 !!!
* 2025.12:  ğŸš€ Selected as Tencent **Qingyun Program** Intern
* 2025.10:  Awarded the **National Scholarship**
* 2025.03:  Awarded the **Tat-Seng Chua Scholarship**
* 2025.03:  ğŸ‰ğŸ‰ 1 paper accepted to CVPR 2025 !!!
<br>

# ğŸ’» Experience
- **Research Intern (Qingyun Program)** \| [Hunyuan3D, Tencent](https://3d.hunyuan.tencent.com/) \| 12/2025 â€“ Now

- **Visiting Student** \| [IGL-HKUST](https://github.com/IGL-HKUST) \| 12/2024 â€“ 09/2025, Mentored by Prof. [Yuan Liu](https://liuyuan-pal.github.io/)

- **Research Intern** \| HITSZ \| 04/2024 â€“ 11/2024, Mentored by Prof. [Baoquan Zhang](https://zhangbq-research.github.io/) and Prof. [Yunming Ye](https://homepage.hit.edu.cn/yeyunming)
  
<br>

# ğŸ“ Publications 
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2026</div><img src='images/e-dit-teaser.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<a href="https://arxiv.org/pdf/2602.13993" style="font-size: 22px; color: #483D8B; text-decoration: none">**Elastic Diffusion Transformer**</a><br>
<span style="font-size: 18px;">Jiangshan Wang, Zeqiang Lai\*, **Jiarui Chen**, Jiayi Guo, Hang Guo, Xiu Li, Xiangyu Yue\*, Chunchao Guo\*</span><br>
<span style="font-size: 18px;">[**Paper**](https://arxiv.org/pdf/2509.07021)   [**Code (Coming soon)**](https://github.com/wangjiangshan0725/Elastic-DiT)</span>

<span style="font-size: 18px;"> We present Elastic Diffusion Transformer (E-DiT), which achieves ~2Ã— acceleration by dynamically allocating computation (depth and width) based on sample-dependent sparsity. </span>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2026</div><img src='images/megs-teaser.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<a href="https://megs-2.github.io/" style="font-size: 22px; color: #483D8B; text-decoration: none">**MEGSÂ²: Memory-Efficient Gaussian Splatting via Spherical Gaussians and Unified Pruning**</a><br>
<span style="font-size: 18px;">**Jiarui Chen\***, Yikeng Chen\*, Yingshuang Zou, Ye Huang, Peng Wang, Yuan Liuâ€ , Yujing Sunâ€ , Wenping Wang</span><br>
<span style="font-size: 18px;">[**Website**](https://megs-2.github.io/) [**Paper**](https://arxiv.org/pdf/2509.07021)   [**Code**](https://github.com/IGL-HKUST/MEGS-2)</span>

<span style="font-size: 18px;"> We present MEGSÂ², a novel 3DGS compression framework that shifts the focus from storage reduction to the critical bottleneck of rendering memory, enabling real-time performance on edge devices and in web browsers. </span>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2025</div><img src='images/CDRA-teaser.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<a href="https://cvpr.thecvf.com/virtual/2025/poster/33380" style="font-size: 22px; color: #483D8B; text-decoration: none">**Sensitivity-Aware Efficient Fine-Tuning via Compact Dynamic-Rank Adaptation**</a><br>
<span style="font-size: 18px;">Tianxing Chen, **Jiarui Chen**, Baoquan Zhang\*, Zhehao Yu, Shidong Chen, Rui Ye, Xutao Li, Yunming Ye</span><br>
<span style="font-size: 18px;">[**Paper**](https://cvpr.thecvf.com/virtual/2025/poster/33380)</span>

<span style="font-size: 18px;"> We present CDRA-SPT, a novel PEFT method that harnesses the complementary strengths of selection-based and reparameterization-based approaches. </span>

</div>
</div>

# ğŸ† Honors and Awards
- National Scholarship, 2025. (Top 0.2% nationwide in China & Top 0.4% in HITSZ)
- Tat-Seng Chua Scholarship, Harbin Institute of Technology, Shenzhen, 2025. (8 undergraduates per year)
- First-Prize Academic Scholarship, Harbin Institute of Technology, Shenzhen, 2024. (Top 5%)
- National Second Prize, 19th Challenge Cup Special Competition, 2024. (Team Leader)
- National First Prize, China Undergraduate Mathematical Contest in Modeling, 2023. (Top 0.5%)


